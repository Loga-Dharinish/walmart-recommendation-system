# -*- coding: utf-8 -*-
"""Walmart.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PACN1PWO9Uy5iH2TBxn9RcKCleMJUpnh

# Loading the datasets

The **surprise** library (and some of its compiled modules) were built for NumPy 1.x
"""

#!pip install numpy==1.26.4



# Commented out IPython magic to ensure Python compatibility.
#!git clone https://github.com/Loga-Dharinish/walmart-recommendation-system.git
# %cd /content/walmart-recommendation-system/


import os
import joblib

walmart = 'cleaned_data_for_model/walmart_5000.csv'
clean_data = 'cleaned_data_for_model/clean_data_4090.csv'
trending_products = 'cleaned_data_for_model/trending_products.csv'

files = [walmart, clean_data, trending_products]

print('\n')
for file in files:
    if os.path.exists(file):
        print(f"{file} \nfound!")
    else:
        print(f"{file} not found!")

"""## Connecting the dataframe to the pandas"""

import pandas as pd
import numpy as np

walmart_df = pd.read_csv(walmart)
clean_df = pd.read_csv(clean_data)
trending_df = pd.read_csv(trending_products)

files_df = [walmart_df, clean_df, trending_df]

files_df[0].info()
files_df[1].info()
files_df[2].info()

"""# Building Content-Based-Recommendation System

## Fuzzy Function
"""

from difflib import get_close_matches

def get_closest_product_name(input_name,df):
  df = files_df[1].copy()
  names = df['Name'].dropna().unique()
  matches = get_close_matches(input_name, names, n=1, cutoff=0.4)
  if matches:
    return matches[0]
  else:
    return None

# Example usage
input_name = "Nice n Easy Permanent Color"
closest_name = get_closest_product_name(input_name, df = files_df[0])

print("Closest Match Found:", closest_name)

"""## Importing Libraries

**Pandas**: Data Manipulation

**TfidfBectorizer**: Converts text into numerical vectors based on

    How often a word appear in a product (term frequency)

    How rare that word is across all products (inverse document frequency)

**cosine_similarity**: How similar two text vectors are.

    If closer the cosine vlaue is to 1, the more similar the two products are.
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""## Loading the copy of the Clean_dataframe"""

df = files_df[1].copy()

df.info()

df.head(2)



"""## Combining Text Features

Creating new column that represents product's details by combining multiple descriptive fields

**Category** : Adds broad context

**Brand** : Adds brand-specific association

**Name** : Main product Name

**Description** : Core product description

**Tags** : Packed with keywords
"""

# Filling the missing values with blank strings.

df['Category'] = df['Category'].fillna('')
df['Brand'] = df['Brand'].fillna('')
df['Name'] = df['Name'].fillna('')
df['Description'] = df['Description'].fillna('')
df['Tags'] = df['Tags'].fillna('')

# Combine the features into one.

df['combined_features'] = (df['Name'] + ' ' + df['Category'] + ' ' + ' ' + df['Brand'] + ' ' + df['Tags']+' '+ df['Description'])

df.tail(2)



"""## TF-IDF Vectorization and **saving**

Converting the **combined_featured** into numerical vectors using **TD-IDF**  *(Term Frequency-Inverse Document Frequency)*
"""

tfidf = TfidfVectorizer(stop_words='english') # Removes common words like "the", "is", etc

joblib.dump(tfidf, 'models/tfidf_vectorizer.pkl')

tfidf_matrix = tfidf.fit_transform(df['combined_features'])


print(tfidf_matrix.shape)

"""## Cosine Similarity Matrix"""

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

np.save('models/cosine_sim.npy', cosine_sim)

import zipfile

with zipfile.ZipFile('models/cosine_sim.zip', 'w') as zipf:
    zipf.write('models/cosine_sim.npy')


"""## Recommendation Function

**get_recommendations** function takesa a **product name** and return **top N most similar product**s using **cosine similarity matrix**.
"""

def get_recommendations(product_name, df, cosine_sim, top_n=10, threshold=0.5):
  '''
  product_name: The name of the product
  df: Clean DataFrame with product details
  cosine_sim: Precomputed similarity matrix from TF-IDF
  top_n: How many similar products to return
  threshold: Minimum similarity score (0 to 1)
  '''

  # Reset index to ensure proper alignment
  df = df.reset_index(drop=True)

  # Get index of product
  try:
      idx = df[df['Name'].str.lower() == product_name.lower()].index[0]
  except:
      return f"❌ Product '{product_name}' not found in the dataset."

  # Get cosine similarity scores for the product
  sim_scores = list(enumerate(cosine_sim[idx]))

  # Sort by similarity (highest first)
  sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

  # Filter out the same product and low-score matches
  filtered_scores = [s for s in sim_scores if s[0] != idx and s[1] >= threshold]

  # Get the top N most similar product indices
  top_n_scores = filtered_scores[:top_n]

  # Get the recommended product rows
  result_df = df.iloc[[i for i, _ in top_n_scores]].copy()

  # Add percentage similarity
  result_df['Similarity (%)'] = [f"{round(score * 100, 2)}%" for _, score in top_n_scores]

  # Optional: Also add raw similarity score if needed
  result_df['Similarity_Score'] = [round(score, 4) for _, score in top_n_scores]

  # Ensure ImageURL is present
  if 'ImageURL' not in result_df.columns:
      result_df['ImageURL'] = ""

  # Return selected columns
  return result_df[['Name', 'Brand', 'Category', 'Rating', 'Similarity (%)', 'ImageURL']]


"""## Examples for the Recommendations System"""

get_recommendations(get_closest_product_name("OPI Infinite Shine, Nail Lacquer Nail Polish, Bubble Bath",df), df, cosine_sim)



"""# Collaborative Filtering

**Surprise** is a specialized Python library for building and analyzing recommender systems based on:

**Explicit feedback** (ratings)

**Collaborative filtering model**s like SVD, KNN, NMF

## Installing Surprise
"""

# pip install scikit-surprise

"""## Prepare the DataFrame"""

df = files_df[1].copy()

df.info()

"""Column **Rating** has more than half of its values are 0

Possible Assumptions: it's not an acutal "bad" rating - it likely means missing vlaue.
"""

df['Rating'].value_counts()

cf_df = df[['ID','ProdID','Rating']].copy()

cf_df = cf_df[cf_df['Rating'] > 0]
cf_df.columns = ['userID','itemID','rating']
cf_df.info()

"""## Import Libraries"""

from surprise import Reader, Dataset, SVD
from surprise.model_selection import train_test_split
from surprise import accuracy

"""## Load Data into Surprise Format

**Surprise** needs data in a specific format using the Reader class
"""

reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(cf_df[['userID', 'itemID', 'rating']], reader)

"""## Split the Data for Training and Testing"""

trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

"""## Train the SVD Model (Singular Value Decomposition)"""

model = SVD()
model.fit(trainset)

"""## Evaluate the Model"""

predictions = model.test(testset)
accuracy.rmse(predictions)

"""## Make Predictions for a User"""

model.predict(uid=1705736792, iid=8)  # Example user and product ID

"""## Saving"""

import pickle

with open('models/svd_model.pkl', 'wb') as f:
    pickle.dump(model, f)

"""## Recommend Top-N Products for a User"""

def recommend_for_user(model, user_id, df, top_n=5):
    # Get all unique product IDs
    product_ids = df['ProdID'].unique()

    # Predict rating for each product for this user
    predictions = [model.predict(user_id, pid) for pid in product_ids]

    # Sort by estimated rating
    predictions.sort(key=lambda x: x.est, reverse=True)

    # Take top N
    top_preds = predictions[:top_n]

    # Build result
    result = []
    for pred in top_preds:
        prod_details = df[df['ProdID'] == pred.iid].iloc[0]
        result.append(
            {
            'Product Name': prod_details['Name'],
            'Brand': prod_details['Brand'],
            'Predicted Rating': round(pred.est, 2)
            }
        )
    return pd.DataFrame(result)

recommendations = recommend_for_user(model, user_id=1705736792, df=df, top_n=3)
print(recommendations)



"""#  Hybrid Recommendation System

This Hybrid Reccomendation System will combine two Recommendation System, **Content-Based** Filtering and **Collaborative Filtering**.

## Defining the Hybrid Function
"""

def hybrid_recommendation(user_id,product_name,df, cosine_sim, svd_model,top_n=5):

    # Reset index for the safety
    df = df.reset_index(drop=True)

    # Get product index fromt from the name
    try:
      idx = df[df['Name'].str.lower() == product_name.lower()].index[0]
    except:
      return f"Product: {product_name} not found in the dataset."

    # Contect-based similarity scores
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores,key=lambda x:x[1], reverse=True)
    sim_scores = sim_scores[1:top_n + 10]

    # Combine similarity with collaborative filtering prediction
    hybrid_results = []
    for i, sim in sim_scores:
      try:
        predicted_rating = svd_model.predict(user_id,df.iloc[i]['ProdID']).est
      except:
        predicted_rating = 0
      final_score = (sim*0.5) + (predicted_rating/5.0 * 0.5)
      hybrid_results.append((i,final_score))

    # Sort by final score
    hybrid_result = sorted(hybrid_results,key=lambda x:x[1], reverse=True)[:top_n]

    # Output
    results = []
    for i, score in hybrid_results:
        row = df.iloc[i]
        results.append({
            'Product Name': row['Name'],
            'Brand': row['Brand'],
            'Category': row['Category'],
            'Estimated Rating': round(svd_model.predict(user_id, row['ProdID']).est, 2),
            'Hybrid Score': round(score, 4)
        })

    return pd.DataFrame(results)

"""#Fuzzy Matching for Flexibility

In recommendation systems (especially content-based or hybrid ones), **fuzzy matching** is used to handle real-world inconsistencies in product names or user queries

## Fuzzy Function
"""

from difflib import get_close_matches

def get_closest_product_name(input_name,df):
  df = files_df[1].copy()
  names = df['Name'].dropna().unique()
  matches = get_close_matches(input_name, names, n=1, cutoff=0.4)
  if matches:
    return matches[0]
  else:
    return None

# Example usage
input_name = "Nice n Easy Permanent Color"
closest_name = get_closest_product_name(input_name, df)

print("Closest Match Found:", closest_name)

"""## Example for hybrid recommendation"""

# Example: hybrid recommendation
hybrid_recommendation(
    user_id=1705736792,
    product_name= get_closest_product_name("Nice n Easy Permanent Color",files_df[1]),
    df=df,
    cosine_sim=cosine_sim,
    svd_model=model,
    top_n=5
)





"""Found a bug on the code, Users don't always remember full product names. Need to fix the typo and noisy input data

# Multi-Model System

Multiple ML models to predict **Product Rating** based on the feature like *brand, category, price, description, review, etc*

*   TF-IDF for text features (name + description + tags)
*   Label Encoding for categories
*   Regression Models (like RandomForest, XGBoost, Linear Regression)
*   Ensemble (average) of all models to predict ratings

## Data Preparation

### Loading the dataset and cleaning

#### Coping the dataset
"""

df = files_df[0].copy()

df.info()

df.isnull().sum()

"""Note: **Product Rating** has 2k+ missing values and since it is the target variable.

#### Handle Missing values
"""

df = df.dropna(subset=['Product Rating'])

df.isnull().sum()

df['Product Price'] = df['Product Price'].fillna(df['Product Price'].median())
df['Product Category'] = df['Product Category'].fillna("Miscellaneous")
df['Product Description'] = df['Product Description'].fillna("")

df.isnull().sum()
input_df = df.copy()

"""Every null value have been handled

19 columns in the dataset, but for the *recommendation system* - based on **product rating**, we'll focus on

Product Catogory, Product Brand, Product Name, Product Description, Product Price, Product Reviews Count, Product Rating and Product Tags
"""



"""## Feature Engineering

Preare the data, so the models can learn from it effectively.

### Loading the Packages
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack

"""### Handling Categorical Features

*   Product Category
*   Product Brand

**label Encoding** for tree based models and **One-Hot Encoding** for linear models.
"""

label_enc_cat = LabelEncoder()
label_enc_brand = LabelEncoder()

df['Product Category Enc'] = label_enc_cat.fit_transform(df['Product Category'])
df['Product Brand Enc'] = label_enc_brand.fit_transform(df['Product Brand'])

"""### Handling Text Feature (TF-IDF)

*   Product Name
*   Product Description
*   Product tags
"""

tfidf_name = TfidfVectorizer(stop_words='english', max_features=500)
tfidf_desc = TfidfVectorizer(stop_words='english', max_features=1000)
tfidf_tags = TfidfVectorizer(stop_words='english', max_features=300)

x_name = tfidf_name.fit_transform(df['Product Name'])
x_desc = tfidf_desc.fit_transform(df['Product Description'])
x_tags = tfidf_tags.fit_transform(df['Product Tags'])

"""### Handling Numeiric Features

*   Product Price
*   List Product Reviews Count

**StandardScaler** to normalize the values.
"""

scaler = StandardScaler()

scaled_price = scaler.fit_transform(df[['Product Price']])
scaled_reviews = scaler.fit_transform(df[['Product Reviews Count']])

"""### Final Feature set"""

# Convert dense to sparse before combining
from scipy.sparse import csr_matrix

x_price = csr_matrix(scaled_price)
x_reviews = csr_matrix(scaled_reviews)
x_cat = csr_matrix(df[['Product Category Enc']].values)
x_brand = csr_matrix(df[['Product Brand Enc']].values)

# Combine all features
X = hstack([x_name, x_desc, x_tags, x_price, x_reviews, x_cat, x_brand])

# Target (for example: Product Rating)
y = df['Product Rating'].fillna(0).values  # or handle missing differently

"""## Multi-Model System

### NearestNeighbors For Recommendations

Aim of this model is to show the similar products based features - content-based recommendations.
"""

from sklearn.neighbors import NearestNeighbors

nn_model = NearestNeighbors(metric='cosine', algorithm='brute')
nn_model.fit(X)

distances, indices = nn_model.kneighbors(X[0], n_neighbors=6)
joblib.dump(nn_model, 'models/nearest_neighbors_model.pkl')

print("Recommendations for:", df.iloc[0]['Product Name'])
for i in range(1, 6):  # skip the first one (it's the same product)
    print(f"{i}. {df.iloc[indices[0][i]]['Product Name']}")

"""### Regression Model - RandomForest

Predict product rating based on all features
"""

# from sklearn.ensemble import RandomForestRegressor
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import mean_squared_error, r2_score

# # Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # train model
# model = RandomForestRegressor(n_estimators=100, random_state=42)
# model.fit(X_train, y_train)


# Load the trained model
model = joblib.load('models/random_forest_model.pkl')

# # Make predictions
y_pred = model.predict(X_test)

# # Evaluate
# print("MSE:", mean_squared_error(y_test, y_pred))
# print("R2 Score:", r2_score(y_test, y_pred))


# Display first 10 predictions
print("Predicted values:", y_pred[:10])

# Compare with actual values
print("Actual values:", y_test[:10])

# # Saving
# joblib.dump(model, 'models/random_forest_model.pkl')





"""### Cluster Product - Kmeans

Group similar products together
"""

from sklearn.cluster import KMeans

X_dense = X.toarray()  # This converts sparse matrix to dense

kmeans = KMeans(n_clusters=5, random_state=42)
clusters_labels = kmeans.fit_predict(X_dense)

df['Cluster'] = clusters_labels


sample_cluster = df[df['Cluster'] == 0].head()
sample_cluster[['Product Name', 'Product Category','Product Brand']]

# saving
joblib.dump(kmeans, 'models/kmeans_model.pkl')

for i in range(5):
    print(f"\n🧃 Cluster {i} Sample:")
    print(df[df['Cluster'] == i][['Product Name', 'Product Category', 'Product Brand']].head(3))

def recommend_from_cluster(product_name, df, top_n=5):
    # Find the product
    target_product = df[df['Product Name'].str.lower() == product_name.lower()]

    if target_product.empty:
        return f"❌ Product '{product_name}' not found."

    # Get the cluster of the target product
    cluster_id = target_product['Cluster'].values[0]

    # Get all products from the same cluster
    cluster_products = df[df['Cluster'] == cluster_id]

    # Exclude the target product
    recommendations = cluster_products[cluster_products['Product Name'] != product_name]

    # Return top N similar products
    return recommendations[['Product Name', 'Product Category', 'Product Brand']].head(top_n)

recommend_from_cluster(get_closest_product_name("IMAN Skin Tone Evener BB Crè", input_df), df, top_n=5)

def recommend_similar_products(product_name, df, top_n=5):
    # Find exact match (or fallback)
    if product_name not in df['Product Name'].values:
        print(f"❌ Product '{product_name}' not found.")
        # Show alternatives
        suggestions = df[df['Product Name'].str.contains(product_name[:5], case=False, na=False)]
        print("🔍 Did you mean one of these?\n", suggestions['Product Name'].head(5).tolist())
        return

    # Get product cluster
    product_cluster = df[df['Product Name'] == product_name]['Cluster'].values[0]

    # Filter products from the same cluster
    cluster_df = df[df['Cluster'] == product_cluster]

    # Drop the selected product itself
    cluster_df = cluster_df[cluster_df['Product Name'] != product_name]

    # Return top_n similar products
    return cluster_df[['Product Name', 'Product Category', 'Product Brand']].head(top_n)

recommend_similar_products("Clairol Nice n Easy Permanent Hair Color, Natural Medium Blonde, 1 Kit", df)

